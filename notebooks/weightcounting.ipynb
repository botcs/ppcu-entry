{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.legacy.nn.Module import Module\n",
    "from torch.legacy.nn.utils import clear\n",
    "\n",
    "\n",
    "class SpatialCrossMapLRN_temp(Module):\n",
    "\n",
    "\tdef __init__(self, size, alpha=1e-4, beta=0.75, k=1, gpuDevice=0):\n",
    "\t\tsuper(SpatialCrossMapLRN_temp, self).__init__()\n",
    "\n",
    "\t\tself.size = size\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.beta = beta\n",
    "\t\tself.k = k\n",
    "\t\tself.scale = None\n",
    "\t\tself.paddedRatio = None\n",
    "\t\tself.accumRatio = None\n",
    "\t\tself.gpuDevice = gpuDevice\n",
    "\n",
    "\tdef updateOutput(self, input):\n",
    "\t\tassert input.dim() == 4\n",
    "\n",
    "\t\tif self.scale is None:\n",
    "\t\t\tself.scale = input.new()\n",
    "\t\t\t\n",
    "\t\tif self.output is None:\n",
    "\t\t\tself.output = input.new()\n",
    "\n",
    "\t\tbatchSize = input.size(0)\n",
    "\t\tchannels = input.size(1)\n",
    "\t\tinputHeight = input.size(2)\n",
    "\t\tinputWidth = input.size(3)\n",
    "\n",
    "\t\tif input.is_cuda:\t\n",
    "\t\t\tself.output = self.output.cuda(self.gpuDevice)\n",
    "\t\t\tself.scale = self.scale.cuda(self.gpuDevice)\n",
    "\n",
    "\t\tself.output.resize_as_(input)\n",
    "\t\tself.scale.resize_as_(input)\n",
    "\n",
    "\t\t# use output storage as temporary buffer\n",
    "\t\tinputSquare = self.output\n",
    "\t\ttorch.pow(input, 2, out=inputSquare)\n",
    "\n",
    "\t\tprePad = int((self.size - 1) / 2 + 1)\n",
    "\t\tprePadCrop = channels if prePad > channels else prePad\n",
    "\n",
    "\t\tscaleFirst = self.scale.select(1, 0)\n",
    "\t\tscaleFirst.zero_()\n",
    "\t\t# compute first feature map normalization\n",
    "\t\tfor c in range(prePadCrop):\n",
    "\t\t\tscaleFirst.add_(inputSquare.select(1, c))\n",
    "\n",
    "\t\t# reuse computations for next feature maps normalization\n",
    "\t\t# by adding the next feature map and removing the previous\n",
    "\t\tfor c in range(1, channels):\n",
    "\t\t\tscalePrevious = self.scale.select(1, c - 1)\n",
    "\t\t\tscaleCurrent = self.scale.select(1, c)\n",
    "\t\t\tscaleCurrent.copy_(scalePrevious)\n",
    "\t\t\tif c < channels - prePad + 1:\n",
    "\t\t\t\tsquareNext = inputSquare.select(1, c + prePad - 1)\n",
    "\t\t\t\tscaleCurrent.add_(1, squareNext)\n",
    "\n",
    "\t\t\tif c > prePad:\n",
    "\t\t\t\tsquarePrevious = inputSquare.select(1, c - prePad)\n",
    "\t\t\t\tscaleCurrent.add_(-1, squarePrevious)\n",
    "\n",
    "\t\tself.scale.mul_(self.alpha / self.size).add_(self.k)\n",
    "\n",
    "\t\ttorch.pow(self.scale, -self.beta, out=self.output)\n",
    "\t\tself.output.mul_(input)\n",
    "\n",
    "\t\treturn self.output\n",
    "\n",
    "\tdef updateGradInput(self, input, gradOutput):\n",
    "\t\tassert input.dim() == 4\n",
    "\n",
    "\t\tbatchSize = input.size(0)\n",
    "\t\tchannels = input.size(1)\n",
    "\t\tinputHeight = input.size(2)\n",
    "\t\tinputWidth = input.size(3)\n",
    "\n",
    "\t\tif self.paddedRatio is None:\n",
    "\t\t\tself.paddedRatio = input.new()\n",
    "\t\tif self.accumRatio is None:\n",
    "\t\t\tself.accumRatio = input.new()\n",
    "\t\tself.paddedRatio.resize_(channels + self.size - 1, inputHeight, inputWidth)\n",
    "\t\tself.accumRatio.resize_(inputHeight, inputWidth)\n",
    "\n",
    "\t\tcacheRatioValue = 2 * self.alpha * self.beta / self.size\n",
    "\t\tinversePrePad = int(self.size - (self.size - 1) / 2)\n",
    "\n",
    "\t\tself.gradInput.resize_as_(input)\n",
    "\t\ttorch.pow(self.scale, -self.beta, out=self.gradInput).mul_(gradOutput)\n",
    "\n",
    "\t\tself.paddedRatio.zero_()\n",
    "\t\tpaddedRatioCenter = self.paddedRatio.narrow(0, inversePrePad, channels)\n",
    "\t\tfor n in range(batchSize):\n",
    "\t\t\ttorch.mul(gradOutput[n], self.output[n], out=paddedRatioCenter)\n",
    "\t\t\tpaddedRatioCenter.div_(self.scale[n])\n",
    "\t\t\ttorch.sum(self.paddedRatio.narrow(0, 0, self.size - 1), 0, out=self.accumRatio)\n",
    "\t\t\tfor c in range(channels):\n",
    "\t\t\t\tself.accumRatio.add_(self.paddedRatio[c + self.size - 1])\n",
    "\t\t\t\tself.gradInput[n][c].addcmul_(-cacheRatioValue, input[n][c], self.accumRatio)\n",
    "\t\t\t\tself.accumRatio.add_(-1, self.paddedRatio[c])\n",
    "\n",
    "\t\treturn self.gradInput\n",
    "\n",
    "\tdef clearState(self):\n",
    "\t\tclear(self, 'scale', 'paddedRatio', 'accumRatio')\n",
    "\t\treturn super(SpatialCrossMapLRN_temp, self).clearState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LambdaBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LambdaBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, input):\n",
    "        output = []\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(input))\n",
    "        return output if output else input\n",
    "\n",
    "\n",
    "class Lambda(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return self.lambda_func(self.forward_prepare(input))\n",
    "\n",
    "\n",
    "#\n",
    "def Conv2d(in_dim, out_dim, kernel, stride, padding):\n",
    "    l = torch.nn.Conv2d(in_dim, out_dim, kernel, stride=stride, padding=padding)\n",
    "    return l\n",
    "\n",
    "def BatchNorm(dim):\n",
    "    l = torch.nn.BatchNorm2d(dim)\n",
    "    return l\n",
    "\n",
    "def CrossMapLRN(size, alpha, beta, k=1.0, gpuDevice=0):\n",
    "    lrn = SpatialCrossMapLRN_temp(size, alpha, beta, k, gpuDevice=gpuDevice)\n",
    "    n = Lambda( lambda x,lrn=lrn: Variable(lrn.forward(x.data).cuda(gpuDevice)) if x.data.is_cuda else Variable(lrn.forward(x.data)) )\n",
    "    return n\n",
    "\n",
    "def Linear(in_dim, out_dim):\n",
    "    l = torch.nn.Linear(in_dim, out_dim)\n",
    "    return l\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, inputSize, kernelSize, kernelStride, outputSize, reduceSize, pool, useBatchNorm, reduceStride=None, padding=True):\n",
    "        super(Inception, self).__init__()\n",
    "        #\n",
    "        self.seq_list = []\n",
    "        self.outputSize = outputSize\n",
    "\n",
    "        #\n",
    "        # 1x1 conv (reduce) -> 3x3 conv\n",
    "        # 1x1 conv (reduce) -> 5x5 conv\n",
    "        # ...\n",
    "        for i in range(len(kernelSize)):\n",
    "            od = OrderedDict()\n",
    "            # 1x1 conv\n",
    "            od['1_conv'] = Conv2d(inputSize, reduceSize[i], (1, 1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n",
    "            if useBatchNorm:\n",
    "                od['2_bn'] = BatchNorm(reduceSize[i])\n",
    "            od['3_relu'] = nn.ReLU()\n",
    "            # nxn conv\n",
    "            pad = int(np.floor(kernelSize[i] / 2)) if padding else 0\n",
    "            od['4_conv'] = Conv2d(reduceSize[i], outputSize[i], kernelSize[i], kernelStride[i], pad)\n",
    "            if useBatchNorm:\n",
    "                od['5_bn'] = BatchNorm(outputSize[i])\n",
    "            od['6_relu'] = nn.ReLU()\n",
    "            #\n",
    "            self.seq_list.append(nn.Sequential(od))\n",
    "\n",
    "        ii = len(kernelSize)\n",
    "        # pool -> 1x1 conv\n",
    "        od = OrderedDict()\n",
    "        od['1_pool'] = pool\n",
    "        if ii < len(reduceSize) and reduceSize[ii] is not None:\n",
    "            i = ii\n",
    "            od['2_conv'] = Conv2d(inputSize, reduceSize[i], (1,1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n",
    "            if useBatchNorm:\n",
    "                od['3_bn'] = BatchNorm(reduceSize[i])\n",
    "            od['4_relu'] = nn.ReLU()\n",
    "        #\n",
    "        self.seq_list.append(nn.Sequential(od))\n",
    "        ii += 1\n",
    "\n",
    "        # reduce: 1x1 conv (channel-wise pooling)\n",
    "        if ii < len(reduceSize) and reduceSize[ii] is not None:\n",
    "            i = ii\n",
    "            od = OrderedDict()\n",
    "            od['1_conv'] = Conv2d(inputSize, reduceSize[i], (1,1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n",
    "            if useBatchNorm:\n",
    "                od['2_bn'] = BatchNorm(reduceSize[i])\n",
    "            od['3_relu'] = nn.ReLU()\n",
    "            self.seq_list.append(nn.Sequential(od))\n",
    "\n",
    "        self.seq_list = nn.ModuleList(self.seq_list)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "\n",
    "        ys = []\n",
    "        target_size = None\n",
    "        depth_dim = 0\n",
    "        for seq in self.seq_list:\n",
    "            #print(seq)\n",
    "            #print(self.outputSize)\n",
    "            #print('x_size:', x.size())\n",
    "            y = seq(x)\n",
    "            y_size = y.size()\n",
    "            #print('y_size:', y_size)\n",
    "            ys.append(y)\n",
    "            #\n",
    "            if target_size is None:\n",
    "                target_size = [0] * len(y_size)\n",
    "            #\n",
    "            for i in range(len(target_size)):\n",
    "                target_size[i] = max(target_size[i], y_size[i])\n",
    "            depth_dim += y_size[1]\n",
    "\n",
    "        target_size[1] = depth_dim\n",
    "        #print('target_size:', target_size)\n",
    "\n",
    "        for i in range(len(ys)):\n",
    "            y_size = ys[i].size()\n",
    "            pad_l = int((target_size[3] - y_size[3]) // 2)\n",
    "            pad_t = int((target_size[2] - y_size[2]) // 2)\n",
    "            pad_r = target_size[3] - y_size[3] - pad_l\n",
    "            pad_b = target_size[2] - y_size[2] - pad_t\n",
    "            ys[i] = F.pad(ys[i], (pad_l, pad_r, pad_t, pad_b))\n",
    "\n",
    "        output = torch.cat(ys, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class netOpenFace(nn.Module):\n",
    "    def __init__(self, useCuda, gpuDevice=0):\n",
    "        super(netOpenFace, self).__init__()\n",
    "\n",
    "        self.gpuDevice = gpuDevice\n",
    "        self.layer1 = Conv2d(3, 64, (7,7), (2,2), (3,3))\n",
    "        self.layer2 = BatchNorm(64)\n",
    "        self.layer3 = nn.ReLU()\n",
    "        self.layer4 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))\n",
    "        self.layer5 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)\n",
    "        self.layer6 = Conv2d(64, 64, (1,1), (1,1), (0,0))\n",
    "        self.layer7 = BatchNorm(64)\n",
    "        self.layer8 = nn.ReLU()\n",
    "        self.layer9 = Conv2d(64, 192, (3,3), (1,1), (1,1))\n",
    "        self.layer10 = BatchNorm(192)\n",
    "        self.layer11 = nn.ReLU()\n",
    "        self.layer12 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)\n",
    "        self.layer13 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))\n",
    "        self.layer14 = Inception(192, (3,5), (1,1), (128,32), (96,16,32,64), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n",
    "        self.layer15 = Inception(256, (3,5), (1,1), (128,64), (96,32,64,64), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n",
    "        self.layer16 = Inception(320, (3,5), (2,2), (256,64), (128,32,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n",
    "        self.layer17 = Inception(640, (3,5), (1,1), (192,64), (96,32,128,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n",
    "        self.layer18 = Inception(640, (3,5), (2,2), (256,128), (160,64,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n",
    "        self.layer19 = Inception(1024, (3,), (1,), (384,), (96,96,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n",
    "        self.layer21 = Inception(736, (3,), (1,), (384,), (96,96,256), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n",
    "        self.layer22 = nn.AvgPool2d((3,3), stride=(1,1), padding=(0,0))\n",
    "        self.layer25 = Linear(736, 128)\n",
    "\n",
    "        #\n",
    "        self.resize1 = nn.UpsamplingNearest2d(scale_factor=3)\n",
    "        self.resize2 = nn.AvgPool2d(4)\n",
    "\n",
    "        #\n",
    "        # self.eval()\n",
    "\n",
    "        if useCuda:\n",
    "            self.cuda(gpuDevice)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "\n",
    "        if x.data.is_cuda and self.gpuDevice != 0:\n",
    "            x = x.cuda(self.gpuDevice)\n",
    "\n",
    "        #\n",
    "        if x.size()[-1] == 128:\n",
    "            x = self.resize2(self.resize1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.layer19(x)\n",
    "        x = self.layer21(x)\n",
    "        x = self.layer22(x)\n",
    "        x = x.view((-1, 736))\n",
    "\n",
    "        x_736 = x\n",
    "\n",
    "        x = self.layer25(x)\n",
    "        x_norm = torch.sqrt(torch.sum(x**2, 1) + 1e-6)\n",
    "        x = torch.div(x, x_norm.view(-1, 1).expand_as(x))\n",
    "\n",
    "        return (x, x_736)\n",
    "\n",
    "\n",
    "def prepareOpenFace(useCuda=True, gpuDevice=0, useMultiGPU=False):\n",
    "    model = netOpenFace(useCuda, gpuDevice)\n",
    "\n",
    "    if useMultiGPU:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_net = netOpenFace(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_n_parameters = sum([p.data.nelement() for p in orig_net.parameters()])\n",
    "print('  + Number of params: {}'.format(orig_n_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.features = torchvision.models.squeezenet1_1(pretrained=True).features\n",
    "            self.embedding = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.Dropout(),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 128)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(-1, 512)\n",
    "            return self.embedding(x)\n",
    "new_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_parameters = sum([p.data.nelement() for p in new_net.parameters()])\n",
    "print('  + Number of params: {}'.format(new_n_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameter complexity ratio (orig/new):', orig_n_parameters / new_n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.cuda.FloatTensor(1, 3, 480, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_net = orig_net.cuda() # ~600MB on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = new_net.cuda() # ~2MB on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit orig_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit new_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time complexity ratio (orig/new):', 17.5 / 3.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
