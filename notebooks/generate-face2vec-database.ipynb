{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import sys\n",
    "import numpy\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "containing_dir = '.'\n",
    "\n",
    "fileDir = os.path.dirname(containing_dir)\n",
    "modelDir = os.path.join(fileDir, 'weights')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dlibFacePredictor', type=str, help=\"Path to dlib's face predictor.\",\n",
    "                    default=os.path.join(modelDir, \"shape_predictor_68_face_landmarks.dat\"))\n",
    "parser.add_argument('--database', type=str, help='Compare query image to pictures found in [database]',\n",
    "                    default='~/Pictures/Webcam/')\n",
    "parser.add_argument('--imgDim', type=int,\n",
    "                    help=\"Default image dimension.\", default=96)\n",
    "parser.add_argument('--verbose', action='store_true')\n",
    "parser.add_argument('--dlib_dim', type=int, help='im size for face recognition', default=224)\n",
    "parser.add_argument('--query_path', default='', help='query image path')\n",
    "parser.add_argument('--refresh', type=int, help='Refresh output image [sec]')\n",
    "parser.add_argument('--webcam', type=int, default=0, help='use webcam')\n",
    "\n",
    "args = parser.parse_args('')\n",
    "align = utils.AlignDlib(args.dlibFacePredictor)\n",
    "prepareOpenFace = utils.prepareOpenFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReadImage(imgPath):\n",
    "\n",
    "    if args.verbose:\n",
    "        print(\"Processing {}.\".format(imgPath))\n",
    "    bgrImg = cv2.imread(imgPath)\n",
    "    if args.verbose:\n",
    "        print(\"Loaded {}.\".format(imgPath))\n",
    "    \n",
    "    if bgrImg is None:\n",
    "        raise Exception(\"Unable to load image: {}\".format(imgPath))\n",
    "    \n",
    "    rgbImg = cv2.cvtColor(bgrImg, cv2.COLOR_BGR2RGB)\n",
    "    if args.verbose:\n",
    "        print(\"Converted {}.\".format(imgPath))\n",
    "    \n",
    "    return rgbImg\n",
    "\n",
    "def ProcessImage(rgbImg, max_ratio=1, returnBB=False):\n",
    "\n",
    "    \n",
    "    start = time.time()\n",
    "    bb = align.getLargestFaceBoundingBox(rgbImg)\n",
    "#     while bb is None and max_ratio > ratio:\n",
    "#         ratio += 0.1\n",
    "#         rgbImg = cv2.resize(orig_rgbImg, None, fx=ratio, fy=ratio)\n",
    "#         bb = align.getLargestFaceBoundingBox(rgbImg)\n",
    "\n",
    "    if args.verbose:\n",
    "        print(\"  + Original size: {}\".format(rgbImg.shape))\n",
    "\n",
    "    if bb is None:\n",
    "        raise RuntimeWarning(\"Unable to find a face!\")\n",
    "    if args.verbose:\n",
    "        print(\"  + Face detection took {} seconds.\".format(time.time() - start))\n",
    "    #bb /= ratio\n",
    "\n",
    "    alignedFace = align.align(args.imgDim, rgbImg, bb,\n",
    "                              landmarkIndices=utils.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "    if alignedFace is None:\n",
    "        raise Exception(\"Unable to align image: {}\".format(imgPath))\n",
    "    if args.verbose:\n",
    "        print(\"  + Aligned size: {}\".format(alignedFace.shape))\n",
    "        print(\"  + Aligned mean: {}\".format(alignedFace.mean()))\n",
    "        print(\"  + Aligned dev: {}\".format(alignedFace.std()))\n",
    "        print(\"  + Face alignment took {} seconds.\".format(time.time() - start))\n",
    "\n",
    "    img = numpy.transpose(alignedFace, (2, 0, 1))\n",
    "    img = img.astype(numpy.float32) / 255.0\n",
    "    #cv2.imshow('preproc', cv2.cvtColor(alignedFace, cv2.COLOR_BGR2RGB))\n",
    "    #print(numpy.min(img), numpy.max(img))\n",
    "    #print(numpy.sum(img[0]), numpy.sum(img[1]), numpy.sum(img[2]))\n",
    "    I_ = torch.from_numpy(img).unsqueeze(0)\n",
    "    if useCuda:\n",
    "        I_ = I_.cuda()\n",
    "    if returnBB:\n",
    "        return I_, utils.rect_to_bb(bb, ratio)\n",
    "    return I_\n",
    "\n",
    "def find_k_nearest(query_im, k=3):\n",
    "    q_var = Variable(query_im, requires_grad=False)\n",
    "    q_f, q_res = model(q_var)\n",
    "    lin_d = ((f_736-q_res)**2).mean(-1)\n",
    "\n",
    "    cos_d = torch.mm(f_736, q_res.transpose(0, 1)).squeeze()\n",
    "    cos_d /= (f_736**2).mean()\n",
    "    #print(lin_d.shape, cos_d.shape)\n",
    "\n",
    "\n",
    "    #dist = ((f - q_f)**2).mean(-1)\n",
    "    dist = lin_d\n",
    "    ds, idxs = dist.topk(k, largest=False)\n",
    "    ds = ds.data.cpu()\n",
    "    idxs = idxs.data.cpu()\n",
    "    #print(ds)\n",
    "    #print(idxs)\n",
    "    #for idx, d in zip(idxs, ds):\n",
    "    #    print('%30s  distance: %0.4f' % (img_paths[idx].split('/')[-1], d))\n",
    "        #torch.dot()\n",
    "    return idxs, ds\n",
    "\n",
    "def draw_bb(frame, bb):\n",
    "    x, y, w, h = bb\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (200, 200, 200), 2)\n",
    "\n",
    "def draw_text(frame, bb, text, distance, threshold=0.0100,\n",
    "              x_offset=0, y_offset=0, font_scale=2, thickness=2):\n",
    "    x, y = bb[:2]\n",
    "    color = (0, 200, 0)\n",
    "    if distance > threshold:\n",
    "        color = (0, 0, 200)\n",
    "        text = 'Ismeretlen'\n",
    "\n",
    "    cv2.putText(frame, text, (x + x_offset, y + y_offset),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "if False:\n",
    "    #\n",
    "    useCuda = True\n",
    "    if useCuda:\n",
    "        assert torch.cuda.is_available()\n",
    "    else:\n",
    "        assert False, 'Sorry, .pth file contains CUDA version of the network only.'\n",
    "\n",
    "    model = prepareOpenFace()\n",
    "    model.load_state_dict(torch.load(os.path.join(containing_dir, 'weights', 'openface.pth')))\n",
    "    model = model.eval()\n",
    "\n",
    "\n",
    "    #img_paths = glob.glob('/home/botoscs/Pictures/office_badges/*.jpg', recursive=True)\n",
    "    #img_paths += glob.glob('/home/botoscs/Pictures/Webcam/*.jpg', recursive=True)\n",
    "    img_paths = glob.glob(args.database + '/**/*.jpg', recursive=True)\n",
    "    imgs = []\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = ReadImage(img_path)\n",
    "            img = ProcessImage(img)\n",
    "            \n",
    "            imgs.append(img)\n",
    "        except RuntimeWarning as w:\n",
    "            continue\n",
    "\n",
    "    I_ = torch.cat(imgs, 0)\n",
    "    I_ = Variable(I_, requires_grad=False)\n",
    "    start = time.time()\n",
    "    f, f_736 = model(I_)\n",
    "    print(\"  + Forward pass took {} seconds.\".format(time.time() - start))\n",
    "\n",
    "\n",
    "    if len(args.query_path) == 0:\n",
    "        cap = cv2.VideoCapture(args.webcam)\n",
    "        acc_idxs = {}\n",
    "\n",
    "        imlist = []\n",
    "        for img_path in img_paths:\n",
    "            imlist.append(cv2.imread(img_path))\n",
    "\n",
    "        cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "        while True:\n",
    "            ret, bgrImg = cap.read()\n",
    "\n",
    "            try:\n",
    "                query_im, rect = ProcessImage(bgrImg, -1, returnBB=True)\n",
    "\n",
    "                idxs, ds = find_k_nearest(query_im, 5)\n",
    "                text = img_paths[idxs[0]].split('/')[-2]\n",
    "                draw_bb(bgrImg, rect)\n",
    "                draw_text(bgrImg, rect, text, ds[0])\n",
    "                if args.verbose:\n",
    "                    #print('\\x1b[2J')\n",
    "                    for idx, d in zip(idxs, ds):\n",
    "                        print('%30s  distance: %0.4f' % (text, d))\n",
    "                for idx in idxs:\n",
    "                    if acc_idxs.get(idx) is None:\n",
    "                        acc_idxs[idx] = 1\n",
    "                    else:\n",
    "                        acc_idxs[idx] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "            finally:\n",
    "                cv2.imshow('frame', bgrImg)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        q_path = args.query_path\n",
    "        query_im = ProcessImage(ReadImage(q_path))\n",
    "        find_k_nearest(query_im, 3)\n",
    "\n",
    "    # in OpenFace's sample code, cosine distance is usually used for f (128d).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCuda = True\n",
    "if useCuda:\n",
    "    assert torch.cuda.is_available()\n",
    "else:\n",
    "    assert False, 'Sorry, .pth file contains CUDA version of the network only.'\n",
    "\n",
    "model = prepareOpenFace()\n",
    "model.load_state_dict(torch.load(os.path.join(containing_dir, 'weights', 'openface.pth')))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.database = '/home/csbotos/video/stable_synced/stable_recordings/camdir-0/2018-04-10/2018-04-10.17/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_paths = glob.glob(args.database + '/**/*.jpg', recursive=True)\n",
    "print('Total images to be loaded: ', len(img_paths))\n",
    "imgs = []\n",
    "img_path = img_paths[0]\n",
    "print(len(imgs), img_path)\n",
    "\n",
    "img = ReadImage(img_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imgs = []\n",
    "true_paths = []\n",
    "for img_path in img_paths:\n",
    "    try:\n",
    "        img = ReadImage(img_path)\n",
    "        img = ProcessImage(img)\n",
    "\n",
    "        imgs.append(img)\n",
    "        true_paths.append(img_path)\n",
    "    except RuntimeWarning as w:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths[-1000:-999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "true_paths = []\n",
    "i = 0\n",
    "for img_path in img_paths[-2000:]:\n",
    "    try:\n",
    "        img = ReadImage(img_path)\n",
    "        img = ProcessImage(img)\n",
    "\n",
    "        imgs.append(img)\n",
    "        true_paths.append(img_path)\n",
    "        i += 1\n",
    "        if i % (len(img_paths) // 30) == 0: print('[%6d : %6d]'%(i, len(img_paths)))\n",
    "    except RuntimeWarning as w:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.verbose = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def ProcessSingle(img_path):\n",
    "    \n",
    "    print('ok?')\n",
    "    try:\n",
    "        img = ReadImage(img_path)\n",
    "        img = ProcessImage(img)\n",
    "        return img\n",
    "    except RuntimeWarning as w:\n",
    "        return None\n",
    "p = multiprocessing.Pool(4)\n",
    "processed_imgs = list(p.map(ProcessSingle, img_paths))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_ = torch.cat([x for x in imgs if x.shape == img.shape])\n",
    "\n",
    "I_ = Variable(I_, requires_grad=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "f, f_736 = model(I_)\n",
    "print(\"  + Forward pass took {} seconds.\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape, f_736.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'fnames':true_paths,\n",
    "    'f_128':f,\n",
    "    'f_736':f_736\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, 'Example2000Database.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "detector(img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "detector(img, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "detector(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "detector(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align.getAllFaceBoundingBoxes(img))\n",
    "print(detector(img, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align.getLargestFaceBoundingBox(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = torch.load('Example2000Database.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool TSNE plots - without labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from sklearn.manifold import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 16, 10\n",
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X736 = database['f_736'].data.cpu().numpy()\n",
    "X128 = database['f_128'].data.cpu().numpy()\n",
    "#X_embedded = TSNE(n_components=2).fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded128 = TSNE(n_components=2, perplexity=20, n_jobs=40, n_iter=1000, cheat_metric=False, metric='cosine').fit_transform(X128[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded736 = TSNE(n_components=2, perplexity=20, n_jobs=40, n_iter=1000, cheat_metric=False, metric='cosine').fit_transform(X736[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "plt.title(\"Embedded face2vec dim: 736\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1])\n",
    "plt.title(\"Embedded face2vec dim: 128\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 790\n",
    "end = 800\n",
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1])\n",
    "plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='r')\n",
    "plt.title(\"Embedded face2vec dim: 128\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_paths.index('/home/csbotos/video/stable_synced/stable_recordings/camdir-0/2018-04-10/2018-04-10.17/2018-04-10.17-45/hasface-2018-04-10.17-45-00.047788.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1490\n",
    "end = 1540\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 128 Feature space')\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 736 Feature space')\n",
    "plt.show()\n",
    "for idx in range(start, end):\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "    plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded128[idx, 0], X_embedded128[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 128 Feature space')\n",
    "    plt.subplot(222)\n",
    "    plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "    plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded736[idx, 0], X_embedded736[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 736 Feature space')\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.imshow(ReadImage(true_paths[idx]))\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(imgs[idx].cpu()[0].permute(1, 2, 0))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 595\n",
    "end = 605\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 128 Feature space')\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 736 Feature space')\n",
    "plt.show()\n",
    "for idx in range(start, end):\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "    plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded128[idx, 0], X_embedded128[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 128 Feature space')\n",
    "    plt.subplot(222)\n",
    "    plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "    plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded736[idx, 0], X_embedded736[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 736 Feature space')\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.imshow(ReadImage(true_paths[idx]))\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(imgs[idx][0].permute(1, 2, 0))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 20\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 128 Feature space')\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "plt.title('Selected instances int the 736 Feature space')\n",
    "plt.show()\n",
    "for idx in range(start, end):\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1], )\n",
    "    plt.scatter(X_embedded128[start:end, 0], X_embedded128[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded128[idx, 0], X_embedded128[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 128 Feature space')\n",
    "    plt.subplot(222)\n",
    "    plt.scatter(X_embedded736[:, 0], X_embedded736[:, 1])\n",
    "    plt.scatter(X_embedded736[start:end, 0], X_embedded736[start:end, 1], c='orange')\n",
    "    plt.scatter(X_embedded736[idx, 0], X_embedded736[idx, 1], c='r', marker='x', s=200, linewidth=5)\n",
    "    plt.title('X marks the instance in the 736 Feature space')\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.imshow(ReadImage(true_paths[idx]))\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(imgs[idx][0].permute(1, 2, 0))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1])\n",
    "plt.title(\"Embedded face2vec dim: 128\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded128[:, 0], X_embedded128[:, 1])\n",
    "plt.title(\"Embedded face2vec dim: 128\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded128_3d = TSNE(n_components=3, perplexity=10).fit_transform(X128[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ipv.quickscatter(X_embedded128_3d[:, 0], X_embedded128_3d[:, 1], X_embedded128_3d[:, 2], size=1, marker=\"sphere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
