{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TripletImageLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import dlib\n",
    "import numpy as np\n",
    "from utils import prepareOpenFace\n",
    "from utils import send_query, send_large_query\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as im\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload filenames to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkstamp(date):\n",
    "    parts = date.split('.')\n",
    "    if len(parts) == 4:\n",
    "        timestamp = int(parts[-2]) * 1000  + int(parts[-1][:3])\n",
    "\n",
    "    else:\n",
    "        dt = datetime.datetime.strptime(date, \"%Y-%m-%d.%H-%M-%S.%f\")\n",
    "        timestamp = time.mktime(dt.timetuple()) + (dt.microsecond / 1000000.0)\n",
    "    \n",
    "        timestamp *= 1000\n",
    "    \n",
    "    return int(timestamp)\n",
    "\n",
    "def get_ts(path):\n",
    "    img_name = os.path.basename(path)\n",
    "    ts = mkstamp(img_name[img_name.find('2018-'):-5])\n",
    "    return ts\n",
    "\n",
    "batch_size = 4500\n",
    "paths = open('aligned.txt').read().splitlines()\n",
    "for idx in range(0, len(paths), batch_size):\n",
    "    FULL_QUERY = ''\n",
    "    for path in paths[idx:idx+batch_size]:\n",
    "        ts = get_ts(path)\n",
    "        SINGLE_SQL_QUERY_STRING =\\\n",
    "        'INSERT INTO ALIGNED(PATH, TIMESTAMP) VALUES(\"%s\", %d); '\\\n",
    "        %(path, ts)\n",
    "        FULL_QUERY += SINGLE_SQL_QUERY_STRING\n",
    "    send_query(FULL_QUERY, verbose=False)\n",
    "    print('[%5d : %5d]'%(min(idx+batch_size, len(paths)), len(paths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download complete, sorted path database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = send_large_query('SELECT path FROM aligned ORDER BY aligned_ID', \n",
    "                                batch_size=50000, verbose=False)\n",
    "database_paths = [q['path'] for q in query_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a batch image loader that _FETCHES_ the pre-aligned faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        super(customDataset, self).__init__()\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "dataset = customDataset(\n",
    "    paths=database_paths,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(96),\n",
    "        transforms.CenterCrop(96),\n",
    "        transforms.ToTensor(),\n",
    "    ]), \n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, batch_size=1024, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the face-embedding network\n",
    "### make sure that it will be optimized for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = prepareOpenFace()\n",
    "net.load_state_dict(torch.load('weights/openface.pth'))\n",
    "net = net.eval()\n",
    "net.cuda()\n",
    "for p in net.parameters():\n",
    "    p.requires_grad = False\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = None\n",
    "for batch_idx, imgs in enumerate(dataloader, 1):\n",
    "    #torch.cuda.empty_cache()\n",
    "    X = Variable(imgs, volatile=True, requires_grad=False).cuda()\n",
    "    if embs is None:\n",
    "        embs = net(X)[0]\n",
    "    else:\n",
    "        embs = torch.cat([embs, net(X)[0]])\n",
    "    print('[%5d|%5d]'%(batch_idx, len(dataloader)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the embeddings database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_database = {\n",
    "    'paths': dataset.paths,\n",
    "    'embeddings': embs.cpu()\n",
    "}\n",
    "\n",
    "torch.save(embedding_database, 'ALIGNED_EMBEDDING_DATABASE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.autograd.Variable(next(thumb_iter).cuda(), volatile=True, requires_grad=False)\n",
    "embeddings_128, embeddings_736 = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThumb(x):\n",
    "    thumb = x.data.cpu().numpy()\n",
    "    thumb = np.array(255 * thumb.transpose(1, 2, 0), dtype='uint8')\n",
    "    return Image.fromarray(thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotwithMargin(anchor_idx, margin=0.1, only_correct=True):\n",
    "    anchor_embedding = embeddings[anchor_idx].expand_as(embeddings)\n",
    "    distance = ((embeddings-anchor_embedding)**2).mean(-1)\n",
    "    for i, (x, d) in enumerate(zip(X, distance)):\n",
    "        print(i, d.data[0], d.data[0] < margin)\n",
    "        if only_correct and d.data[0] < margin:\n",
    "            display(getThumb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.data - torch.ones(1, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotwithMargin(250, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKclosest(anchor_idx, k):\n",
    "    anchor_embedding = embeddings[anchor_idx].expand_as(embeddings)\n",
    "    distance = ((embeddings-anchor_embedding)**2).mean(-1)\n",
    "    idxs = torch.sort(distance)[1][:k]\n",
    "    for i in range(k):\n",
    "        print(i, idxs[i].data[0], distance[idxs[i]].data[0])\n",
    "        display(getThumb(X[idxs[i].data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKclosest(117000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TripletImageLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as im\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_database = torch.load('ALIGNED_EMBEDDING_DATABASE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = embedding_database['embeddings']\n",
    "paths = embedding_database['paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKclosest(anchor_idx, k):\n",
    "    anchor_embedding = embs[anchor_idx].expand_as(embs)\n",
    "    distance = ((embs-anchor_embedding)**2).mean(-1)\n",
    "    idxs = torch.sort(distance)[1][:k]\n",
    "    for i in range(k):\n",
    "        print(i, idxs[i].data[0], distance[idxs[i]].data[0])\n",
    "        \n",
    "        display(Image.open(dataset.paths[idxs[i].data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.paths.index('/home/csbotos/video/stable_synced/stable_recordings/camdir-0/2018-04-10/2018-04-10.16/2018-04-10.16-15/aligned/hasface-2018-04-10.16-15-54.000441.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import send_query, send_large_query\n",
    "\n",
    "query_params = {\n",
    "    'minusoffset': 2000,\n",
    "    'plusoffset': 2000,\n",
    "}\n",
    "\n",
    "SQL_QUERY = '''\n",
    "    SELECT aligned_ID, path, name FROM aligned JOIN (\n",
    "        SELECT name, timestamp-{minusoffset} as start, timestamp+{plusoffset} as end \n",
    "        FROM Mandacsko_log WHERE gate = \"Forgóvilla jobb (kintről) BE\") \n",
    "    ON aligned.timestamp BETWEEN start AND end;\n",
    "'''\n",
    "SQL_QUERY = SQL_QUERY.format(**query_params)\n",
    "\n",
    "aligned_ID_name_path = send_query(SQL_QUERY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There could be cases when a path is assigned to multiple users\n",
    "aligned_ID_names = {}\n",
    "path_aligned_ID = {}\n",
    "for q in aligned_ID_name_path:\n",
    "    if aligned_ID_names.get(q['aligned_ID']) is None:\n",
    "        aligned_ID_names[int(q['aligned_ID'])] = [q['name']]\n",
    "    else:\n",
    "        aligned_ID_names[int(q['aligned_ID'])].append(q['name'])\n",
    "        \n",
    "    path_aligned_ID[q['path']] = int(q['aligned_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_ID_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.dataset.paths = list(path_aligned_ID.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, imgs in enumerate(dataloader, 1):\n",
    "    #torch.cuda.empty_cache()\n",
    "    X = Variable(imgs, volatile=True, requires_grad=False).cuda()\n",
    "    if embs is None:\n",
    "        embs = net(X)[0]\n",
    "    else:\n",
    "        embs = torch.cat([embs, net(X)[0]])\n",
    "    print('[%5d|%5d]'%(batch_idx, len(dataloader)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for embedded_ID, path in enumerate(dataloader.dataset.paths):\n",
    "    if path_aligned_ID.get(path) is not None:\n",
    "        names.append(aligned_ID_names[path_aligned_ID[path]])\n",
    "    else:\n",
    "        print('FUUUUCK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The i_th embedding in the N x 128 array corresponds to the i_th array of the [first] registered name\n",
    "names = [aligned_ID_names[aligned_ID][0] for aligned_ID in path_aligned_ID.values()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_embeddings = {\n",
    "    'names': names,\n",
    "    'embeddings': embs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(registered_embeddings, 'registered_embeddings.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in aligned_ID_names.values() if len(x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedded_ID_names), len(dataloader.dataset.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_ID_names[117000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_ID_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vector_db = {\n",
    "    'embedded_ID_aligned_ID': embedded_ID_aligned_ID,\n",
    "    'aligned'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding = embs[48666].expand_as(embs)\n",
    "distance = pdist(embs, anchor_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(distance):\n",
    "    if d.data[0] < 0.2:\n",
    "        print(d.data[0], d.data[0]<0.2)\n",
    "        display(Image.open(dataset.paths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding = embs[48637:48645]\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "distance = pdist(anchor_embedding, anchor_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(48637, 48645):\n",
    "    display(Image.open(dataset.paths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix = torch.sum((anchor_embedding[:, None, :] - anchor_embedding[None, :, :]) ** 2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dmatrix.data)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.PairwiseDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TripletImageLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    TripletImageLoader(\n",
    "        'name_photoPaths_database.csv', \n",
    "        transform=transforms.Compose([\n",
    "            transforms.CenterCrop(480),\n",
    "            transforms.ToTensor(),\n",
    "        ])),\n",
    "    batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 0\n",
    "for anchor_batch, distant_batch, similar_batch in dl:\n",
    "    print(anchor_batch.size(), flush=True)\n",
    "    i += 1\n",
    "    if i>15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = [\"asdasd\" for _ in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tester[np.random.choice(len(tester))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit random.choice(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = Variable(torch.stack(next(iter(dl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = Variable(torch.stack(next(iter(dl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
